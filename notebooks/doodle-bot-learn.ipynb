{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doodle-bot-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a code along with the talk I gave on the use of GANs to generate images.\n",
    "Here this is done using the MNIST handwritten digits dataset.\n",
    "\n",
    "Much of this is drawn from the tensorflow models tutorials, which can be found [here](https://github.com/tensorflow/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _WARNING:_\n",
    "\n",
    "#### _Due to file size limitations, I was not able to push the tensorflow dependencies that were included in the project. The two dependencies are [slim](https://github.com/tensorflow/models/tree/master/research/slim) and [mnist](https://github.com/tensorflow/models/tree/master/research/gan/mnist). In order to run this notebook, copy those directories, and place them in this repo._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CGAN.py           \u001b[34massets\u001b[m\u001b[m            \u001b[34mslide_deck\u001b[m\u001b[m        visualizer.py\r\n",
      "README.md         evaluate.py       \u001b[34mslim\u001b[m\u001b[m              xent_score.png\r\n",
      "__init__.py       \u001b[34mmnist\u001b[m\u001b[m             tf_nets.py\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m       \u001b[34mnotebooks\u001b[m\u001b[m         training_loss.png\r\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')  # go to main folder\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom modules found in project\n",
    "import visualizer\n",
    "import evaluate\n",
    "import tf_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e1cb3b92910c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# mnist examples from `tensorflow/models`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/repos/doodle-bot-learn/mnist/data_provider.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_factory\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mslim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/repos/doodle-bot-learn/slim/datasets/dataset_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflowers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimagenet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/repos/doodle-bot-learn/slim/datasets/cifar10.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mslim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# mnist examples from `tensorflow/models`.\n",
    "from mnist import data_provider\n",
    "from mnist import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-slim data provider\n",
    "from slim.datasets import download_and_convert_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"assets/data/\"\n",
    "RESULT_DIR = \"assets/results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short cuts\n",
    "tfgan = tf.contrib.gan\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input pipeline\n",
    "if not tf.gfile.Exists(DATA_DIR):  # check if data directory already exists\n",
    "    tf.gfile.MakeDirs(DATA_DIR)\n",
    "\n",
    "download_and_convert_mnist.run(DATA_DIR)  # download data if missing\n",
    "\n",
    "batch_size = 32\n",
    "with tf.device('/cpu:0'):  # pin it to the cpu and save gpu for propagation\n",
    "    images, one_hot_labels, _ = data_provider.provide_data('train', batch_size, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the images\n",
    "imgs_to_visualize = tfgan.eval.image_reshaper(images[:20,...], num_cols=10)\n",
    "visualizer.image(imgs_to_visualize, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dims = 64  # shape of noise generation\n",
    "\n",
    "conditional_gan_model = tfgan.gan_model(generator_fn=tf_nets.generator,\n",
    "                                        discriminator_fn=tf_nets.discriminator,\n",
    "                                        real_data=images,\n",
    "                                        generator_inputs=(tf.random_normal([batch_size, noise_dims]),one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pre-training generator images\n",
    "cond_generated_data_to_visualize = tfgan.eval.image_reshaper(conditional_gan_model.generated_data[:20,...], num_cols=10)\n",
    "visualizer.image(cond_generated_data_to_visualize, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tfgan.gan_loss(conditional_gan_model, gradient_penalty_weight=1.0)\n",
    "evaluate.gan_loss(loss)  # test loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.train.AdamOptimizer(0.0009, beta1=0.5)  # instantiate optimizers\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(0.00009, beta1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_train_ops = tfgan.gan_train_ops(conditional_gan_model, loss, generator_optimizer, discriminator_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_eval = 500\n",
    "assert images_to_eval % 10 == 0  # ensure multiples of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_noise = tf.random_normal([images_to_eval, 64])\n",
    "one_hot_labels = tf.one_hot([i for _ in range(images_to_eval // 10) for i in range(10)], depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(conditional_gan_model.generator_scope, reuse=True):\n",
    "    eval_images = conditional_gan_model.generator_fn((random_noise, one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_eval_imgs = tfgan.eval.image_reshaper(eval_images[:20, ...], num_cols=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pretrained classifier to save on training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_CLASSIFIER_FROZEN_GRAPH = 'mnist/data/classify_mnist_graph_def.pb'\n",
    "xent_score = util.mnist_cross_entropy(eval_images, one_hot_labels, MNIST_CLASSIFIER_FROZEN_GRAPH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "train_step_fn = tfgan.get_sequential_train_steps()\n",
    "loss_values, xent_score_values = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()  # instantiate saver\n",
    "    sess.run(tf.global_variables_initializer())  # run!\n",
    "\n",
    "    with slim.queues.QueueRunners(sess):        \n",
    "        start_time = time.time()  # start timer\n",
    "        for i in range(3001):  # number of steps - reduced from 500 for run time\n",
    "            cur_loss, _ = train_step_fn(sess, gan_train_ops, global_step, train_step_kwargs={})\n",
    "            loss_values.append((i, cur_loss))\n",
    "\n",
    "            if not i % 10:\n",
    "                xent_score_values.append((i, sess.run(xent_score)))\n",
    "\n",
    "            if not i % 100:\n",
    "                print(f'Current loss: {cur_loss:.2f}')\n",
    "                print(f'Current cross entropy score: {xent_score_values[-1][1]:.2f}')\n",
    "                visualizer.generated_image(i, start_time, sess.run(reshaped_eval_imgs), save=True)\n",
    "\n",
    "        #  program complete\n",
    "        \n",
    "        save_path = saver.save(sess, \"assets/saved_models/model.ckpt\")\n",
    "        print(f\"Model saved in file: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cross entropy scores\n",
    "plt.title('Cross Entropy Score Per Step')\n",
    "plt.plot(*zip(*xent_score_values))\n",
    "plt.savefig('cross_entropy.png', dpi=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Training Loss Per Step')\n",
    "plt.plot(*zip(*loss_values))\n",
    "plt.savefig('loss.png', dpi=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
